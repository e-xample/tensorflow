{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST For ML Beginners\n",
    "https://www.tensorflow.org/versions/r0.10/tutorials/mnist/beginners/index.html#mnist-for-ml-beginners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST データをダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 手書き文字画像はこんな感じ\n",
    "28px * 28px のグレースケール画像。<br>\n",
    "今回のタスクは、この画像を入力としてどの数字であるかを識別すること。\n",
    "<img src=\"MNIST_For_ML_Beginners/MNIST.png\" width=\"500px\">\n",
    "このままじゃコンピュータは読めないので、、、値域0~1で符号化します\n",
    "<img src=\"MNIST_For_ML_Beginners/MNIST-Matrix.png\" width=\"500px\">\n",
    "\n",
    "\n",
    "### データセットの構成\n",
    "学習用画像データセット：55,000枚 (mnist.train)<br>\n",
    "テスト用画像データセット：10,000枚 (mnist.test)<br>\n",
    "\n",
    "## モデルにどう入力するか、モデルからどう出力するか\n",
    "\n",
    "### 入力の設計\n",
    "今回は28行28列のベクトルを、単純に784行1列のベクトルに変換して入力ベクトルとする。\n",
    "<img src=\"MNIST_For_ML_Beginners/mnist-train-xs.png\" width=\"500px\">\n",
    "\n",
    "### 出力(正解ラベル)の設計\n",
    "問題<br>\n",
    "識別器は何を出力したら、良さげでしょうか？？<br>\n",
    "その場合、正解ラベルはどのように表現すればいいでしょうか？？<br>\n",
    "\n",
    "ベストプラクティス<br>\n",
    "出力：各クラスに識別される確率の分布<br>\n",
    "正解ラベル：one-hot-vector\n",
    "\n",
    "識別クラス数と同じ要素数のベクトルで、クラスkが正解である出力ベクトルは、k番目が「1」で、その他が「0」をとるone-hot-vector <br>\n",
    "例えば「3」が正解であるベクトルは [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]T\n",
    "<img src=\"MNIST_For_ML_Beginners/mnist-train-ys.png\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 手書き文字を表現するためのモデルを考える\n",
    "今回は、モデルは２層のニューラルネットワークを扱います。（※これはディープラーニングではありません。）<br>\n",
    "\n",
    "### ニューラルネットワークの構造\n",
    "ニューラルネットワークの構造は以下のとおり（簡単化のために、図は入出力ともに３次元ベクトルで表現している）。<br>\n",
    "「ユニット」と、それらを結ぶ「エッジ」で構成されている。\n",
    "<img src=\"MNIST_For_ML_Beginners/softmax-regression-scalargraph.png\" width=\"500px\">\n",
    "\n",
    "（今回の）ニューラルネットワークは入力層・出力層の２層構造で構成されている。<br>\n",
    "まず入力層のユニットからデータ「x」(784行1列のベクトルデータ)が入力され、エッジを通して次の層のユニットへと伝搬される。<br>\n",
    "それを繰り返して出力層のユニットから最終的な出力「y」(10行1列のベクトルデータ)が得られる。<br>\n",
    "\n",
    "### 減衰\n",
    "エッジにはそれぞれ重み「w」がついており、エッジを通るデータはこのwによって重み付けをされる。<br>\n",
    "さらにバイアス「b」が加算され、次の層のユニットへと伝搬する。<br>\n",
    "また、各ユニットでは活性化関数と呼ばれる関数により、伝搬してきたデータを変換してから次のエッジへ送る。<br>\n",
    "\n",
    "### 活性化関数\n",
    "今回のモデルでは、入力層のユニットの活性化関数として恒等関数、出力層のユニットの活性化関数としてsoftmax関数を用いている。<br>\n",
    "恒等関数とは、各ユニットに入力された値をそのまま出力する関数である。<br>\n",
    "softmax関数は、各ユニットに入力された値を「0~1」の範囲に変換し出力する関数である。<br>\n",
    "さらに、出力層の全ユニットから得られる出力の総和が「1」となるように調整される。<br>\n",
    "softmax関数により、各ユニットから出力される値を、そのユニットが表す識別クラスである「確率」とみなすことができるようになる。<br>\n",
    "（この考え方は、ロジスティック回帰の2値分類を、多クラス分類へと拡張させたものである。）<br>\n",
    "\n",
    "これらは人間の脳神経における情報の伝搬と記憶の原理をモデル化したものであるため、ニューラルネットワーク（神経回路）モデルと呼ぶ。<br>\n",
    "\n",
    "### 数式に落とし込む\n",
    "入力xとエッジの付けられた重みw、活性化関数(softmax関数)、出力yを用いると、上記のモデルは以下の様に表現できる。\n",
    "<img src=\"MNIST_For_ML_Beginners/softmax-regression-scalarequation.png\" width=\"500px\">\n",
    "\n",
    "これは行列と（列）ベクトルを用いて、以下の様に表現できる。\n",
    "<img src=\"MNIST_For_ML_Beginners/softmax-regression-vectorequation.png\" width=\"500px\">\n",
    "\n",
    "重みwの行列を「W」とすれば、以下の様な式で書ける。<br>\n",
    "$y = \\text{softmax}(Wx + b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 実装しちゃおう！\n",
    "\n",
    "### まずはモデルの定義から\n",
    "以上で述べたモデルは、Tensor Flowを使えば簡単に書けちゃいます<br>\n",
    "まずは、ニューラルネットワークを構成する入力と出力について定義する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "続いて、重みとバイアスを定義する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "そして、さきほど説明した数式$y = \\text{softmax}(Wx + b)$を作る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = tf.nn.softmax(tf.matmul(x, W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 誤差を定義しよう！（cross-entropy関数）\n",
    "\n",
    "これで推定値yを求めることできる道具と、手元には正解ラベルy_が揃いました・・・つまり、学習できる！！！\n",
    "\n",
    "学習するとは、「推定値と正解の誤差（誤差関数、損失関数）を最小化するような、重みwとバイアスbを求める」ということ。<br>\n",
    "なので、学習するためには、まず誤差を定義してあげなきゃだめ<br>\n",
    "\n",
    "今回は、損失関数にcross-entropy関数を用いる。<br>\n",
    "yを推定値（１０行１列の確率分布）、y'を正解ラベル（１０行１列のone-hot-vector）とすると、<br>\n",
    "$H_{y'}(y) = -\\sum_i y'_i \\log(y_i)$\n",
    "\n",
    "なぜcross-entropy関数を損失関数として使っているのかについては、下記本のp.16-p.20にわかりやすく書かれているので気になる人はチェック！<br>\n",
    "ロジスティック回帰と同じ仕組みなので、それから勉強すると、good<br>\n",
    "\n",
    "「深層学習」岡谷貴之、機械学習プロフェッショナルシリーズ<br>\n",
    "https://www.amazon.co.jp/dp/B018K6C99A/ref=dp-kindle-redirect?_encoding=UTF8&btkr=1\n",
    "\n",
    "<img src=\"MNIST_For_ML_Beginners/dl.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 誤差を最小化したい！勾配降下法\n",
    "\n",
    "誤差を定義できたら、いよいよそれを最小化してあげれば良い。<br>\n",
    "ニューラルネットワークには、この誤差を最小化する方法が確立されている。<br>\n",
    "勾配降下法と呼ばれるものだ。<br>\n",
    "\n",
    "Tensor Flowには、それを計算してくれる関数が用意されてるので、さっきの誤差関数をぶっ込めばok！！！<br>\n",
    "\n",
    "理解したくば、「深層学習」p.23-p.54ページをなめ回すように読もう！（３章：確率的勾配降下法、４章：誤差逆伝搬法）<br>\n",
    "\n",
    "このような最適化アルゴリズムはdeep learningのような学習モデルとは独立したもの。<br>\n",
    "deep learningを設計する上ではあまり考えなくても良いと思います。<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(learning_rate=0.5).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learning_rateとは「１回データを与えた時の学習効果をどれほどにするか」という調整を行うハイパーパラメータ。<br>\n",
    "あの人は話を盛るから話半分くらいに受け止めよう、みたいなことを実現するもの"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習しよう！\n",
    "\n",
    "もう手札は揃った。\n",
    "いざ学習じゃあああ！\n",
    "\n",
    "ところで、一気に全てのデータを使って学習する（バッチ学習）のは計算が大変なんだよね。。<br>\n",
    "小さいデータセットを作って学習する（ミニバッチ学習）方法をとるのが効率的！<br>\n",
    "これを確率的勾配降下法と呼びます。確率的なのは、小さいデータセットを作る時ランダムにデータを選んでいるから。<br>\n",
    "ミニバッチのサイズは10~100が良いとされている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100個のデータを使った学習を、1000回繰り返し行っています。<br>\n",
    "ディープラーニングでは、この繰り返し回数を「エポックサイズ」と呼んでいます。<br>\n",
    "重みや損失関数の収束具合を見て適切なエポックサイズを選んであげる必要があります。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 結果はどうなのよ\n",
    "学習したモデルを評価しましょう。<br>\n",
    "\n",
    "当然ですが、どのように評価するかをまず定義しなければなりません。<br>\n",
    "１行目の式は、推定値と正解ラベルのargmaxが一致しているかどうかをbooleanで返す、というものを定義しています。<br>\n",
    "２行目で、正解率を算出するものを定義しています。<br>\n",
    "３行目で、実際にテストデータを使って計算しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9151\n"
     ]
    }
   ],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 約92% !!!\n",
    "\n",
    "めでたし。めでたし。\n",
    "\n",
    "、、、とはならない！！！\n",
    "\n",
    "続く、チュートリアルにはこの結果について以下のように書かれています。<br>\n",
    "「Getting 92% accuracy on MNIST is bad. It's almost embarrassingly bad.」<br>\n",
    "\n",
    "## （意訳）「92%程度で喜んでんじゃねーぞks!」\n",
    "\n",
    "ということなので、続くチュートリアルをこなして、99%台を目指しましょう！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
