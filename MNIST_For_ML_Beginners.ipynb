{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST データをダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 手書き文字画像はこんな感じ\n",
    "28px * 28px のグレースケール画像。今回のタスクがこの画像を入力として、どの数字であるかを識別すること。\n",
    "<img src=\"MNIST_For_ML_Beginners/MNIST.png\" width=\"500px\">\n",
    "このままじゃコンピュータは読めないので、、、ベクトル化！\n",
    "<img src=\"MNIST_For_ML_Beginners/MNIST-Matrix.png\" width=\"500px\">\n",
    "\n",
    "\n",
    "## データセットの構成\n",
    "学習用画像データセット：55,000枚 (mnist.train)<br>\n",
    "テスト用画像データセット：10,000枚 (mnist.test)<br>\n",
    "\n",
    "### 入力\n",
    "今回は28行28列のベクトルを、単純に784行1列のベクトルに変換して入力ベクトルとする。\n",
    "<img src=\"MNIST_For_ML_Beginners/mnist-train-xs.png\" width=\"500px\">\n",
    "\n",
    "### 出力(正解ラベル)\n",
    "識別クラス数と同じ要素数のベクトルで、クラスkが正解である出力ベクトルは、k番目が「1」で、その他が「0」をとるone-hot-vector <br>\n",
    "例えば「3」が正解であるベクトルは [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]T\n",
    "<img src=\"MNIST_For_ML_Beginners/mnist-train-ys.png\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 手書き文字を表現するためのモデルを考える\n",
    "今回は、モデルは２層のニューラルネットワークを扱います。<br>\n",
    "\n",
    "### ニューラルネットワークの構造\n",
    "ニューラルネットワークの構造は以下のとおり（簡単化のために、図は入出力ともに３次元ベクトルで表現している）。<br>\n",
    "「ユニット」と、それらを結ぶ「エッジ」で構成されている。\n",
    "<img src=\"MNIST_For_ML_Beginners/softmax-regression-scalargraph.png\" width=\"500px\">\n",
    "\n",
    "（今回の）ニューラルネットワークは入力層・出力層の２層構造で構成されている。<br>\n",
    "まず入力層のユニットからデータ「x」(784行1列のベクトルデータ)が入力され、エッジを通して次の層のユニットへと伝搬される。<br>\n",
    "それを繰り返して出力層のユニットから最終的な出力「y」(10行1列のベクトルデータ)が得られる。<br>\n",
    "\n",
    "エッジにはそれぞれ重み「w」がついており、エッジを通るデータはこのwによって重み付けをされる。<br>\n",
    "さらにバイアス「b」が加算され、次の層のユニットへと伝搬する。<br>\n",
    "また、各ユニットでは活性化関数と呼ばれる関数により、伝搬してきたデータを変換してから次のエッジへ送る。<br>\n",
    "\n",
    "これらは人間の脳神経における情報の伝搬と記憶の原理をモデル化したものであるため、ニューラルネットワーク（神経回路）モデルと呼ぶ。<br>\n",
    "\n",
    "### 活性化関数\n",
    "今回のモデルでは、入力層のユニットの活性化関数として恒等関数、出力層のユニットの活性化関数としてsoftmax関数を用いている。<br>\n",
    "恒等関数とは、各ユニットに入力された値をそのまま出力する関数である。<br>\n",
    "softmax関数は、各ユニットに入力された値を「0~1」の範囲に変換し出力する関数である。<br>\n",
    "さらに、出力層の全ユニットから得られる出力の総和が「1」となるように調整される。<br>\n",
    "softmax関数により、各ユニットから出力される値を、そのユニットが表す識別クラスである「確率」とみなすことができるようになる。<br>\n",
    "（この考え方は、ロジスティック回帰の2値分類を、多クラス分類へと拡張させたものである。）<br>\n",
    "\n",
    "### 数式に落とし込む\n",
    "入力xとエッジの付けられた重みw、活性化関数(softmax関数)、出力yを用いると、上記のモデルは以下の様に表現できる。\n",
    "<img src=\"MNIST_For_ML_Beginners/softmax-regression-scalarequation.png\" width=\"500px\">\n",
    "\n",
    "これは行列とベクトルを用いて、以下の様に表現できる。\n",
    "<img src=\"MNIST_For_ML_Beginners/softmax-regression-vectorequation.png\" width=\"500px\">\n",
    "\n",
    "重みwの行列を「W」とすれば、以下の様な式で書ける。<br>\n",
    "$y = \\text{softmax}(Wx + b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 実装しちゃおう！\n",
    "\n",
    "### まずはモデルの定義から\n",
    "以上で述べたモデルは、Tensor Flowを使えばこんなに簡単に書ける！<br>\n",
    "まずは、ニューラルネットワークを構成する入力層と出力層について定義する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "続いて、重みとバイアスを定義する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "そして、さきほど説明した数式$y = \\text{softmax}(Wx + b)$を作る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = tf.nn.softmax(tf.matmul(x, W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 誤差を定義しよう！（cross-entropy関数）\n",
    "\n",
    "これで推定値yを求めることできる道具と、手元には正解ラベルy_がある・・・つまり学習できる！！！\n",
    "\n",
    "学習するとは、推定値と正解の誤差（誤差関数、損失関数）を最小化するような、重みwとバイアスbを求める、ということ。<br>\n",
    "なので、学習するためには、まず誤差を定義してあげなきゃだめ<br>\n",
    "\n",
    "今回は、損失関数にcross-entropy関数を用いる。<br>\n",
    "yを推定値（１０行１列の確率分布）、y'を正解ラベル（１０行１列のone-hot-vector）とすると、<br>\n",
    "$H_{y'}(y) = -\\sum_i y'_i \\log(y_i)$\n",
    "\n",
    "なぜcross-entropy関数を損失関数として使っているのかについては、下記本のp.16-p.20にわかりやすく書かれているので気になる人はチェック！<br>\n",
    "\n",
    "「深層学習」岡谷貴之、機械学習プロフェッショナルシリーズ\n",
    "\n",
    "<img src=\"MNIST_For_ML_Beginners/dl.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 誤差を最小化してくれるええ奴！（勾配降下法）\n",
    "\n",
    "誤差を定義できたら、いよいよそれを最小化してあげれば良い。<br>\n",
    "ニューラルネットワークには、この誤差を最小化する方法が確立されている。<br>\n",
    "勾配降下法と呼ばれるものだ。<br>\n",
    "\n",
    "Tensor Flowには、それを計算してくれる関数が用意されてるので、さっきの誤差関数をぶっ込めばok！！！<br>\n",
    "\n",
    "理解したくば、「深層学習」p.23-p.54ページをなめ回すように読もう！\n",
    "（３章：確率的勾配降下法、４章：誤差逆伝搬法）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習しよう！\n",
    "\n",
    "もう手札は揃った。\n",
    "いざ学習じゃあああ！\n",
    "\n",
    "一気に全てのデータを使って学習する（バッチ学習）のは計算が大変。。<br>\n",
    "小さいデータセットを作って学習する（ミニバッチ学習）方法をとるのが効率的！<br>\n",
    "これを確率的勾配降下法と呼ぶ。確率的なのは、小さいデータセットを作る時ランダムにデータを選んでいるから。<br>\n",
    "ミニバッチのサイズは10~100が良いとされている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 結果はどうなのよ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9178\n"
     ]
    }
   ],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 約92% !!!\n",
    "\n",
    "終"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
